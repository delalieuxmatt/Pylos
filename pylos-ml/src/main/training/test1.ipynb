{
 "cells": [
  {
   "cell_type": "code",
   "id": "4b37695b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T20:25:12.673457Z",
     "start_time": "2025-11-13T20:25:12.668766Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DATASET_PATH = \"resources/games/all_battles.json\"\n",
    "MODEL_EXPORT_PATH = \"resources/models/\"\n",
    "SELECTED_PLAYERS = []\n",
    "DISCOUNT_FACTOR = 0.98\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 512\n",
    "N_CORES = 8\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(N_CORES)\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = str(N_CORES)\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = str(N_CORES)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "#cuda fouten verbergen\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "e7cd4831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T20:25:12.723710Z",
     "start_time": "2025-11-13T20:25:12.719537Z"
    }
   },
   "source": [
    "def build_dataset(path):\n",
    "    \"\"\"\n",
    "    CRITICAL FIX: Generate training data from BOTH players' perspectives.\n",
    "\n",
    "    The key insight: In a two-player zero-sum game, we need to train the model\n",
    "    to evaluate positions from a consistent perspective. We do this by:\n",
    "    1. For each board position, create TWO training examples\n",
    "    2. One from Light's perspective (original board, positive if Light wins)\n",
    "    3. One from Dark's perspective (inverted board, positive if Dark wins)\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"Processing {len(data)} games\")\n",
    "\n",
    "    boards = []\n",
    "    scores = []\n",
    "\n",
    "    for game_idx, game in enumerate(data):\n",
    "        # Skip games that don't have both players in the selected players list\n",
    "        if SELECTED_PLAYERS and (\n",
    "                game[\"lightPlayer\"] not in SELECTED_PLAYERS or\n",
    "                game[\"darkPlayer\"] not in SELECTED_PLAYERS\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        winner = game[\"winner\"]  # 1 for Light, -1 for Dark\n",
    "        n_moves = len(game[\"boardHistory\"])\n",
    "        reserve_size = game['reserveSize']\n",
    "\n",
    "        for i, board_as_long in enumerate(game[\"boardHistory\"]):\n",
    "            # Convert board to array (0 = Light, 1 = Dark)\n",
    "            board_as_array = np.array(\n",
    "                [(board_as_long >> j) & 1 for j in range(59, -1, -1)],\n",
    "                dtype=np.float32\n",
    "            )\n",
    "\n",
    "            # Calculate discount based on proximity to end of game\n",
    "            # Positions closer to the end are more certain\n",
    "            discount = DISCOUNT_FACTOR ** (n_moves - i - 1)\n",
    "\n",
    "            # Light's perspective: positive if Light wins\n",
    "            light_score = winner * discount * reserve_size\n",
    "            boards.append(board_as_array)\n",
    "            scores.append(light_score)\n",
    "\n",
    "            # Dark's perspective: flip the board (0->1, 1->0) and score\n",
    "            # This teaches the model to evaluate from the active player's view\n",
    "            dark_board = 1.0 - board_as_array\n",
    "            dark_score = -winner * discount * reserve_size  # Flip the score\n",
    "            boards.append(dark_board)\n",
    "            scores.append(dark_score)\n",
    "\n",
    "        if game_idx % 1000 == 0:\n",
    "            print(f\"Processed game {game_idx}/{len(data)}\")\n",
    "\n",
    "    boards_array = np.array(boards, dtype=np.float32)\n",
    "    scores_array = np.array(scores, dtype=np.float32)\n",
    "\n",
    "    # Shuffle the dataset to mix Light and Dark perspectives\n",
    "    indices = np.random.permutation(len(boards_array))\n",
    "\n",
    "    return boards_array[indices], scores_array[indices]\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "365f11d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T20:25:12.771335Z",
     "start_time": "2025-11-13T20:25:12.767951Z"
    }
   },
   "source": [
    "def build_dense_model( units1=256, units2=128, units3=64, units4=32, dropout1=0.3, dropout2=0.3, dropout3=0.2, ):\n",
    "     inputs = layers.Input(shape=(60,), dtype=tf.float32) \n",
    "     x = layers.Dense(units1, activation='relu')(inputs) \n",
    "     x = layers.BatchNormalization()(x)\n",
    "     x = layers.Dropout(dropout1)(x) \n",
    "     x = layers.Dense(units2, activation='relu')(x) \n",
    "     x = layers.BatchNormalization()(x) \n",
    "     x = layers.Dropout(dropout2)(x)\n",
    "     x = layers.Dense(units3, activation='relu')(x) \n",
    "     x = layers.BatchNormalization()(x)\n",
    "     x = layers.Dropout(dropout3)(x) \n",
    "     x = layers.Dense(units4, activation='relu')(x) \n",
    "     x = layers.BatchNormalization()(x) \n",
    "     outputs = layers.Dense(1)(x) \n",
    "     model = models.Model(inputs=inputs, outputs=outputs) \n",
    "     return model "
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "9d38bfcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T20:25:12.819284Z",
     "start_time": "2025-11-13T20:25:12.815801Z"
    }
   },
   "source": [
    "def build_conv1d_model(\n",
    "    filters1=32,\n",
    "    filters2=64,\n",
    "    kernel_size=3,\n",
    "    dense_units1=64,\n",
    "    dense_units2=32,\n",
    "    dropout=0.3,\n",
    "):\n",
    "    inputs = layers.Input(shape=(60,), dtype=tf.float32)\n",
    "\n",
    "    x = layers.Reshape((60, 1))(inputs)  # (batch, 60, 1)\n",
    "\n",
    "    x = layers.Conv1D(filters1, kernel_size=kernel_size,\n",
    "                      activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(filters2, kernel_size=kernel_size,\n",
    "                      activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x = layers.Dense(dense_units1, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = layers.Dense(dense_units2, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "ef3e6ad0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T20:25:12.866739Z",
     "start_time": "2025-11-13T20:25:12.864050Z"
    }
   },
   "source": [
    "def create_optimizer(name, learning_rate):\n",
    "    name = name.lower()\n",
    "    if name == \"adam\":\n",
    "        return tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif name == \"sgd\":\n",
    "        return tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    elif name == \"rmsprop\":\n",
    "        return tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer name: {name}\")\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "4763cc46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T20:25:12.916572Z",
     "start_time": "2025-11-13T20:25:12.911992Z"
    }
   },
   "source": [
    "def train_single_experiment(\n",
    "    model_builder,\n",
    "    model_kwargs,\n",
    "    optimizer_name,\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test,\n",
    "    label,\n",
    "):\n",
    "    print(f\"\\n==== Running experiment: {label} ====\")\n",
    "    print(f\"Model kwargs: {model_kwargs}\")\n",
    "    print(f\"Optimizer: {optimizer_name}, lr={learning_rate}, batch_size={batch_size}\")\n",
    "\n",
    "    model = model_builder(**model_kwargs)\n",
    "\n",
    "    optimizer = create_optimizer(optimizer_name, learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=4,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0  # maakt de output niet te spammy\n",
    "    )\n",
    "\n",
    "    test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    print(f\"Best val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "    print(f\"Best val_mae:  {min(history.history['val_mae']):.4f}\")\n",
    "    print(f\"Test loss:     {test_loss:.4f}\")\n",
    "    print(f\"Test MAE:      {test_mae:.4f}\")\n",
    "\n",
    "    result = {\n",
    "        \"label\": label,\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"best_val_loss\": float(min(history.history[\"val_loss\"])),\n",
    "        \"best_val_mae\": float(min(history.history[\"val_mae\"])),\n",
    "        \"history\": history.history,\n",
    "    }\n",
    "\n",
    "    return result\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "4b318204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T20:25:12.968775Z",
     "start_time": "2025-11-13T20:25:12.963671Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "def run_experiments(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    experiments = []\n",
    "\n",
    "    # 1) Dense modellen met verschillende groottes / batch sizes / optimizers\n",
    "    experiments.append({\n",
    "        \"label\": \"dense_big_adam_bs512\",\n",
    "        \"model_builder\": build_dense_model,\n",
    "        \"model_kwargs\": dict(units1=256, units2=128, units3=64, units4=32,\n",
    "                             dropout1=0.3, dropout2=0.3, dropout3=0.2),\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 512,\n",
    "    })\n",
    "\n",
    "    experiments.append({\n",
    "        \"label\": \"dense_small_adam_bs256\",\n",
    "        \"model_builder\": build_dense_model,\n",
    "        \"model_kwargs\": dict(units1=128, units2=64, units3=32, units4=16,\n",
    "                             dropout1=0.3, dropout2=0.3, dropout3=0.2),\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 256,\n",
    "    })\n",
    "\n",
    "    experiments.append({\n",
    "        \"label\": \"dense_big_sgd_bs512\",\n",
    "        \"model_builder\": build_dense_model,\n",
    "        \"model_kwargs\": dict(units1=256, units2=128, units3=64, units4=32,\n",
    "                             dropout1=0.3, dropout2=0.3, dropout3=0.2),\n",
    "        \"optimizer\": \"sgd\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"batch_size\": 512,\n",
    "    })\n",
    "\n",
    "    # 2) Conv1D modellen met verschillende filters / kernel size\n",
    "    experiments.append({\n",
    "        \"label\": \"conv1d_std_adam_bs512\",\n",
    "        \"model_builder\": build_conv1d_model,\n",
    "        \"model_kwargs\": dict(filters1=32, filters2=64,\n",
    "                             kernel_size=3,\n",
    "                             dense_units1=64, dense_units2=32,\n",
    "                             dropout=0.3),\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 512,\n",
    "    })\n",
    "\n",
    "    experiments.append({\n",
    "        \"label\": \"conv1d_wide_adam_bs256\",\n",
    "        \"model_builder\": build_conv1d_model,\n",
    "        \"model_kwargs\": dict(filters1=64, filters2=128,\n",
    "                             kernel_size=5,\n",
    "                             dense_units1=128, dense_units2=64,\n",
    "                             dropout=0.4),\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 256,\n",
    "    })\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for exp in experiments:\n",
    "        res = train_single_experiment(\n",
    "            model_builder=exp[\"model_builder\"],\n",
    "            model_kwargs=exp[\"model_kwargs\"],\n",
    "            optimizer_name=exp[\"optimizer\"],\n",
    "            learning_rate=exp[\"learning_rate\"],\n",
    "            batch_size=exp[\"batch_size\"],\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_val=X_val, y_val=y_val,\n",
    "            X_test=X_test, y_test=y_test,\n",
    "            label=exp[\"label\"],\n",
    "        )\n",
    "        results.append(res)\n",
    "\n",
    "    # tabelletje met overzicht\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            \"label\": r[\"label\"],\n",
    "            \"optimizer\": r[\"optimizer\"],\n",
    "            \"lr\": r[\"learning_rate\"],\n",
    "            \"batch_size\": r[\"batch_size\"],\n",
    "            \"best_val_loss\": r[\"best_val_loss\"],\n",
    "            \"best_val_mae\": r[\"best_val_mae\"],\n",
    "            \"test_loss\": r[\"test_loss\"],\n",
    "            \"test_mae\": r[\"test_mae\"],\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "\n",
    "    print(\"\\n==================== RESULT TABLE ====================\")\n",
    "    print(df.sort_values(\"test_mae\"))\n",
    "\n",
    "    # plots maken\n",
    "    plot_metric_histories(results, metric=\"mae\")\n",
    "    plot_metric_histories(results, metric=\"loss\")\n",
    "\n",
    "    return results, df\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "9ddd347c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T20:25:13.017717Z",
     "start_time": "2025-11-13T20:25:13.014811Z"
    }
   },
   "source": [
    "def plot_metric_histories(results, metric=\"mae\"):\n",
    "    plt.figure()\n",
    "    for r in results:\n",
    "        h = r[\"history\"]\n",
    "        if metric not in h:\n",
    "            continue\n",
    "        plt.plot(h[metric], linestyle=\"-\", label=f\"{r['label']} - train\")\n",
    "        val_key = f\"val_{metric}\"\n",
    "        if val_key in h:\n",
    "            plt.plot(h[val_key], linestyle=\"--\", label=f\"{r['label']} - val\")\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f\"{metric} per epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "4c7634b5",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-13T20:25:13.063079Z"
    }
   },
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "boards, scores = build_dataset(DATASET_PATH)\n",
    "\n",
    "# ---- Train / Val / Test split ----\n",
    "N = len(boards)\n",
    "test_size = int(N * 0.2)\n",
    "val_size  = int((N - test_size) * 0.2)\n",
    "\n",
    "X_test = boards[:test_size]\n",
    "y_test = scores[:test_size]\n",
    "\n",
    "X_trainval = boards[test_size:]\n",
    "y_trainval = scores[test_size:]\n",
    "\n",
    "X_val = X_trainval[:val_size]\n",
    "y_val = y_trainval[:val_size]\n",
    "\n",
    "X_train = X_trainval[val_size:]\n",
    "y_train = y_trainval[val_size:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Val size:   {len(X_val)}\")\n",
    "print(f\"Test size:  {len(X_test)}\")\n",
    "\n",
    "# hyperparameter search\n",
    "results, df = run_experiments(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "# eventueel: beste config tonen\n",
    "best_row = df.sort_values(\"test_mae\").iloc[0]\n",
    "print(\"\\nBest experiment based on test MAE:\")\n",
    "print(best_row)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "Loading dataset...\n",
      "Processing 12000 games\n",
      "Processed game 0/12000\n",
      "Processed game 1000/12000\n",
      "Processed game 2000/12000\n",
      "Processed game 3000/12000\n",
      "Processed game 4000/12000\n",
      "Processed game 5000/12000\n",
      "Processed game 6000/12000\n",
      "Processed game 7000/12000\n",
      "Processed game 8000/12000\n",
      "Processed game 9000/12000\n",
      "Processed game 10000/12000\n",
      "Processed game 11000/12000\n",
      "Train size: 581664\n",
      "Val size:   145415\n",
      "Test size:  181769\n",
      "\n",
      "==== Running experiment: dense_big_adam_bs512 ====\n",
      "Model kwargs: {'units1': 256, 'units2': 128, 'units3': 64, 'units4': 32, 'dropout1': 0.3, 'dropout2': 0.3, 'dropout3': 0.2}\n",
      "Optimizer: adam, lr=0.001, batch_size=512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763065517.043089   97302 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2003 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-11-13 21:25:36.789627: I external/local_xla/xla/service/service.cc:163] XLA service 0x7017c40072c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-13 21:25:36.789661: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2025-11-13 21:25:36.825800: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-13 21:25:37.092676: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91500\n",
      "2025-11-13 21:25:37.236194: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:37.236289: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:37.236308: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:37.236322: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:37.236433: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:38.701625: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1199', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:39.186364: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_358', 428 bytes spill stores, 452 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:39.604359: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2182', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:39.625166: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2258', 124 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:40.012789: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2461', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:40.301320: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2461', 1064 bytes spill stores, 1064 bytes spill loads\n",
      "\n",
      "I0000 00:00:1763065546.322807   97751 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-11-13 21:25:55.572487: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:55.572535: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:55.572543: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:55.572581: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:55.572594: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:55.572601: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:55.572611: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-13 21:25:57.213561: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_358', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:57.368820: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1199', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:58.620282: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2410', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:58.875509: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2410', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:58.968799: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2427', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:58.996985: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2410', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-13 21:25:59.220228: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2444', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
